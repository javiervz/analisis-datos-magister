{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Análisis Computacional de Datos Lingüísticos**\n",
    "### Javier Vera Zúñiga, javier.vera@pucv.cl\n",
    "## A. **ejercicios texto!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lea el archivo  'udhr_arn.txt' en la carpeta **udhr**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recomendación! use:\n",
    "\n",
    "#file = open('udhr/udhr_arn.txt','r', encoding=\"utf8\")\n",
    "#file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## una función útil :)\n",
    "\n",
    "import string\n",
    "\n",
    "def remover_puntuacion(s): \n",
    "    for c in string.punctuation:\n",
    "        s=s.replace(c,\"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resuelva los siguientes problemas de pre-procesamiento:\n",
    "    \n",
    "1. elimine los espacios en blanco que sobran\n",
    "2. Divida cada string de la lista **file** en oraciones. Use **sent_tokenize** de la librería [nltk](https://www.nltk.org/install.html)\n",
    "3. Ahora cada elemento de la lista **file** es una lista de strings. Convierta estos strings en minúscula.\n",
    "4. Use la función **remover_puntuacion** para remover la puntuacion de los strings.\n",
    "5. Divida cada string por espacios en blanco. Ahora la lista **lista** es una lista de listas de strings :)\n",
    "6. Calcule el largo promedio de las oraciones del texto.\n",
    "7. Calcule el número de tokens y el número de types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lea el archivo  'udhr_spa.txt' en la carpeta **udhr**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resuelva los siguientes problemas de pre-procesamiento:\n",
    "    \n",
    "1. Rehaga los pasos 1 y 2 de los ejercicios anteriores.\n",
    "2. Usemos [spacy](https://spacy.io/usage). Instale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (21.2.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (58.0.4)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-58.1.0-py3-none-any.whl (816 kB)\n",
      "Requirement already satisfied: wheel in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (0.37.0)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.0.4\n",
      "    Uninstalling setuptools-58.0.4:\n",
      "      Successfully uninstalled setuptools-58.0.4\n",
      "Successfully installed setuptools-58.1.0\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.1.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp39-cp39-win_amd64.whl (6.5 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy) (21.0)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp39-cp39-win_amd64.whl (451 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy) (1.21.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp39-cp39-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, tqdm, thinc, spacy-legacy, pathy, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 cymem-2.0.5 murmurhash-1.0.5 pathy-0.6.0 preshed-3.0.5 pydantic-1.8.2 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 tqdm-4.62.3 typer-0.4.0 typing-extensions-3.10.0.2 wasabi-0.8.2\n",
      "Collecting es-core-news-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.1.0/es_core_news_sm-3.1.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from es-core-news-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (1.21.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (58.1.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (21.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jxver\\anaconda3\\envs\\geo_env\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->es-core-news-sm==3.1.0) (2.0.1)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.1.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Todos los seres humanos nacen libres e iguales en dignidad y derechos y, dotados como están de razón y conciencia, deben comportarse fraternalmente los unos con los otros.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemmas = []\n",
    "pos = []\n",
    "\n",
    "for token in doc:\n",
    "    tokens+=[token.text]\n",
    "    lemmas+=[token.lemma_], \n",
    "    pos+=[token.pos_] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Todos', 'los', 'seres', 'humanos', 'nacen']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['todo'], ['el'], ['ser'], ['humano'], ['nacen']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'DET', 'NOUN', 'ADJ', 'VERB']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada oración del texto, detecte los sustantivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('udhr/udhr_eng.txt','r', encoding=\"utf8\")\n",
    "file = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita el mismo análisis para el archivo **'udhr_eng.txt'** en la carpeta **udhr**. Agregue la línea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. **ejercicios pandas!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usemos pandas!\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## leamos un archivo csv\n",
    "\n",
    "sources = pd.read_csv('sources_south_america.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>languages</th>\n",
       "      <th>name</th>\n",
       "      <th>family</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abip1241</td>\n",
       "      <td>Abipon</td>\n",
       "      <td>guai1249</td>\n",
       "      <td>long_grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>acha1250</td>\n",
       "      <td>Achagua</td>\n",
       "      <td>araw1281</td>\n",
       "      <td>grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>achu1248</td>\n",
       "      <td>Achuar-Shiwiar</td>\n",
       "      <td>jiva1245</td>\n",
       "      <td>long_grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ache1246</td>\n",
       "      <td>Aché</td>\n",
       "      <td>tupi1275</td>\n",
       "      <td>grammar_sketch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>acro1239</td>\n",
       "      <td>Acroá</td>\n",
       "      <td>nucl1710</td>\n",
       "      <td>wordlist_or_less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>583</td>\n",
       "      <td>yama1264</td>\n",
       "      <td>Yámana</td>\n",
       "      <td>unk</td>\n",
       "      <td>grammar_sketch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>584</td>\n",
       "      <td>yano1269</td>\n",
       "      <td>Yãnoma</td>\n",
       "      <td>yano1268</td>\n",
       "      <td>wordlist_or_less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>585</td>\n",
       "      <td>zamu1245</td>\n",
       "      <td>Zamuco</td>\n",
       "      <td>zamu1243</td>\n",
       "      <td>grammar_sketch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>586</td>\n",
       "      <td>zoee1240</td>\n",
       "      <td>Zo'é</td>\n",
       "      <td>tupi1275</td>\n",
       "      <td>phonology_or_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>587</td>\n",
       "      <td>zapa1253</td>\n",
       "      <td>Záparo</td>\n",
       "      <td>zapa1251</td>\n",
       "      <td>long_grammar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 languages            name    family             source\n",
       "0             0  abip1241          Abipon  guai1249       long_grammar\n",
       "1             1  acha1250         Achagua  araw1281            grammar\n",
       "2             2  achu1248  Achuar-Shiwiar  jiva1245       long_grammar\n",
       "3             3  ache1246            Aché  tupi1275     grammar_sketch\n",
       "4             4  acro1239           Acroá  nucl1710   wordlist_or_less\n",
       "..          ...       ...             ...       ...                ...\n",
       "583         583  yama1264          Yámana       unk     grammar_sketch\n",
       "584         584  yano1269          Yãnoma  yano1268   wordlist_or_less\n",
       "585         585  zamu1245          Zamuco  zamu1243     grammar_sketch\n",
       "586         586  zoee1240            Zo'é  tupi1275  phonology_or_text\n",
       "587         587  zapa1253          Záparo  zapa1251       long_grammar\n",
       "\n",
       "[588 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'languages', 'name', 'family', 'source'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## columnas!\n",
    "\n",
    "sources.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtremos algunas columnas!\n",
    "\n",
    "languages = sources.drop(columns=['Unnamed: 0','source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>languages</th>\n",
       "      <th>name</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abip1241</td>\n",
       "      <td>Abipon</td>\n",
       "      <td>guai1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acha1250</td>\n",
       "      <td>Achagua</td>\n",
       "      <td>araw1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>achu1248</td>\n",
       "      <td>Achuar-Shiwiar</td>\n",
       "      <td>jiva1245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ache1246</td>\n",
       "      <td>Aché</td>\n",
       "      <td>tupi1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acro1239</td>\n",
       "      <td>Acroá</td>\n",
       "      <td>nucl1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>yama1264</td>\n",
       "      <td>Yámana</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>yano1269</td>\n",
       "      <td>Yãnoma</td>\n",
       "      <td>yano1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>zamu1245</td>\n",
       "      <td>Zamuco</td>\n",
       "      <td>zamu1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>zoee1240</td>\n",
       "      <td>Zo'é</td>\n",
       "      <td>tupi1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>zapa1253</td>\n",
       "      <td>Záparo</td>\n",
       "      <td>zapa1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    languages            name    family\n",
       "0    abip1241          Abipon  guai1249\n",
       "1    acha1250         Achagua  araw1281\n",
       "2    achu1248  Achuar-Shiwiar  jiva1245\n",
       "3    ache1246            Aché  tupi1275\n",
       "4    acro1239           Acroá  nucl1710\n",
       "..        ...             ...       ...\n",
       "583  yama1264          Yámana       unk\n",
       "584  yano1269          Yãnoma  yano1268\n",
       "585  zamu1245          Zamuco  zamu1243\n",
       "586  zoee1240            Zo'é  tupi1275\n",
       "587  zapa1253          Záparo  zapa1251\n",
       "\n",
       "[588 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas preguntas:\n",
    "    \n",
    "1. Construya un diccionario que tenga como keys los códigos glottocode, y como values los nombres de las lenguas.\n",
    "2. Construya otro diccionario que tenga como keys los códigos glottocode, y como values las familias de las lenguas.\n",
    "3. Determine todas las familias lingüísticas documentadas. Filtre el dataframe **languages** para alguna familia. \n",
    "4. Cuente el número de lenguas de cada familia.\n",
    "5. Construya el diccionario {familia:[lengua1,lengua2,...],...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
